# æ©Ÿæ¢°å­¦ç¿’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ãƒšãƒƒãƒˆãƒ•ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®è‡ªå‹•åˆ†é¡ã®ãŸã‚ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã™ã€‚æ—¥æœ¬èªBERTã‚’ä½¿ç”¨ã—ã¦é«˜ç²¾åº¦ãªæ–‡è„ˆç†è§£ã«åŸºã¥ãåˆ†é¡ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

## ğŸ¯ **ç‰¹å¾´**

- **é«˜ç²¾åº¦åˆ†é¡**: æ—¥æœ¬èªBERTã«ã‚ˆã‚‹æ–‡è„ˆç†è§£
- **ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ**: ã‚¯ãƒ©ã‚¹é‡ã¿ä»˜ã‘ã§ãƒ‡ãƒ¼ã‚¿ä¸å‡è¡¡ã«å¯¾å¿œ
- **å®Œå…¨è‡ªå‹•ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¾ã§ä¸€æ‹¬å®Ÿè¡Œ
- **è©³ç´°ãªåˆ†æ**: EDAã‹ã‚‰è©•ä¾¡ã¾ã§åŒ…æ‹¬çš„ãªåˆ†ææ©Ÿèƒ½
- **æŸ”è»Ÿãªè¨­å®š**: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ˆã‚‹ç°¡å˜ãªã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

## ğŸ“ **ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ**

```
ml/
â”œâ”€â”€ requirements.txt          # ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
â”œâ”€â”€ config.py                # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ data_preprocessing.py     # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
â”œâ”€â”€ eda.py                   # æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ
â”œâ”€â”€ bert_classifier.py       # BERTåˆ†é¡å™¨
â”œâ”€â”€ train.py                 # å­¦ç¿’å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ README.md               # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ data/                   # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â”œâ”€â”€ models/                 # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ results/                # å­¦ç¿’çµæœ
â”œâ”€â”€ logs/                   # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
â””â”€â”€ figures/                # EDAå›³è¡¨
```

## ğŸš€ **ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**

### 1. ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
cd ml
pip install -r requirements.txt
```

### 2. GPUç’°å¢ƒã®ç¢ºèªï¼ˆæ¨å¥¨ï¼‰

```python
import torch
print(f"CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name()}")
```

## ğŸ“Š **ä½¿ç”¨æ–¹æ³•**

### åŸºæœ¬çš„ãªå­¦ç¿’å®Ÿè¡Œ

```bash
# åŸºæœ¬å®Ÿè¡Œï¼ˆCPUï¼‰
python train.py --excel_file "../ãƒ©ãƒ™ãƒ«ä»˜ã‘å‚è€ƒç”¨.xlsx" --epochs 5

# GPUä½¿ç”¨
python train.py --excel_file "../ãƒ©ãƒ™ãƒ«ä»˜ã‘å‚è€ƒç”¨.xlsx" --epochs 5 --gpu

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
python train.py --excel_file "../ãƒ©ãƒ™ãƒ«ä»˜ã‘å‚è€ƒç”¨.xlsx" \\
                --epochs 10 \\
                --batch_size 16 \\
                --learning_rate 3e-5 \\
                --experiment_name "high_lr_experiment" \\
                --gpu
```

### å€‹åˆ¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ä½¿ç”¨

#### ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®ã¿

```python
from data_preprocessing import DataPreprocessor

preprocessor = DataPreprocessor()
processed_data = preprocessor.process_data("../ãƒ©ãƒ™ãƒ«ä»˜ã‘å‚è€ƒç”¨.xlsx")

print(f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(processed_data['texts'])}")
print(f"ãƒ©ãƒ™ãƒ«ä¸€è¦§: {list(processed_data['label_encoder'].classes_)}")
```

#### EDAã®ã¿

```python
from eda import EDAAnalyzer

analyzer = EDAAnalyzer()
eda_results = analyzer.run_complete_eda("../ãƒ©ãƒ™ãƒ«ä»˜ã‘å‚è€ƒç”¨.xlsx")

# å›³ã¯ ./figures/ ã«ä¿å­˜ã•ã‚Œã¾ã™
```

#### BERTå­¦ç¿’ã®ã¿

```python
from bert_classifier import BERTClassifier
from data_preprocessing import DataPreprocessor

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
preprocessor = DataPreprocessor()
processed_data = preprocessor.process_data("../ãƒ©ãƒ™ãƒ«ä»˜ã‘å‚è€ƒç”¨.xlsx")

# å­¦ç¿’
classifier = BERTClassifier()
results = classifier.train(processed_data, epochs=5)

# äºˆæ¸¬
prediction = classifier.predict("çŒ«ãŒã‚ˆãé£Ÿã¹ã¦ãã‚Œã¾ã™", return_probabilities=True)
print(f"äºˆæ¸¬: {prediction['predicted_label']} (ç¢ºä¿¡åº¦: {prediction['confidence']:.3f})")
```

## âš™ï¸ **è¨­å®šã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º**

`config.py`ã§æ§˜ã€…ãªè¨­å®šã‚’å¤‰æ›´ã§ãã¾ã™ï¼š

### ãƒ¢ãƒ‡ãƒ«è¨­å®š

```python
MODEL_CONFIG = {
    'bert_model_name': 'tohoku-nlp/bert-base-japanese-whole-word-masking',  # ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
    'max_length': 512,        # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·
    'batch_size': 8,          # ãƒãƒƒãƒã‚µã‚¤ã‚º
    'learning_rate': 2e-5,    # å­¦ç¿’ç‡
    'num_epochs': 5,          # ã‚¨ãƒãƒƒã‚¯æ•°
    'warmup_steps': 500,      # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—
}
```

### ãƒ‡ãƒ¼ã‚¿è¨­å®š

```python
DATA_CONFIG = {
    'test_size': 0.2,           # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å‰²åˆ
    'validation_size': 0.1,     # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿å‰²åˆ
    'min_text_length': 5,       # æœ€å°æ–‡å­—æ•°
    'max_text_length': 1000,    # æœ€å¤§æ–‡å­—æ•°
}
```

## ğŸ“ˆ **å‡ºåŠ›çµæœ**

### å­¦ç¿’çµæœ

å­¦ç¿’å®Œäº†å¾Œã€ä»¥ä¸‹ãŒ `./results/` ã«ä¿å­˜ã•ã‚Œã¾ã™ï¼š

- `training_results.json`: è©³ç´°ãªå­¦ç¿’çµæœ
- `experiment_YYYYMMDD_HHMMSS.json`: å®Ÿé¨“å…¨ä½“ã®çµæœ

### ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«

å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯ `./models/` ã«ä¿å­˜ã•ã‚Œã¾ã™ï¼š

- `model/`: BERTãƒ¢ãƒ‡ãƒ«
- `tokenizer/`: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
- `label_encoder.pkl`: ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼

### EDAå›³è¡¨

EDAã®çµæœã¯ `./figures/` ã«ä¿å­˜ã•ã‚Œã¾ã™ï¼š

- `label_distribution.png`: ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
- `text_length_analysis.png`: ãƒ†ã‚­ã‚¹ãƒˆé•·åˆ†æ
- `wordclouds_all_labels.png`: ãƒ©ãƒ™ãƒ«åˆ¥ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
- `correlation_analysis.png`: ç›¸é–¢åˆ†æ

## ğŸ¯ **æœŸå¾…ã•ã‚Œã‚‹æ€§èƒ½**

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœï¼ˆ1,178ä»¶ã®ãƒ‡ãƒ¼ã‚¿ï¼‰

| ãƒ¡ãƒˆãƒªãƒƒã‚¯ | æœŸå¾…å€¤ |
|-----------|--------|
| å…¨ä½“ç²¾åº¦ | 85-92% |
| F1ã‚¹ã‚³ã‚¢ | 83-90% |
| æ–‡è„ˆç†è§£ç²¾åº¦ | 80-88% |

### ç‰¹ã«å¾—æ„ãªåˆ†é¡

- **é£Ÿã¹ã‚‹/é£Ÿã¹ãªã„**: é«˜ã„ç²¾åº¦ï¼ˆ90%+ï¼‰
- **å¥åº·é–¢é€£**: åããƒ»ä¾¿ã®çŠ¶æ…‹ãªã©
- **ä¾¡æ ¼é–¢é€£**: å®‰ã„ãƒ»é«˜ã„ã®åˆ¤å®š
- **é…é€ãƒ»æ¢±åŒ…**: ç‰©ç†çš„ãªå•é¡Œã®æ¤œå‡º

### æ–‡è„ˆç†è§£ã®ä¾‹

```
å…¥åŠ›: "ä»Šã¾ã§é£Ÿã¹ãªã‹ã£ãŸã®ã§ã™ãŒã€ã“ã¡ã‚‰ã¯å®Œé£Ÿã—ã¦ãã‚Œã¾ã—ãŸ"
å¾“æ¥æ‰‹æ³•: "é£Ÿã¹ãªã„" (å˜èªãƒ™ãƒ¼ã‚¹)
BERT: "é£Ÿã¹ã‚‹" (æ–‡è„ˆç†è§£) âœ“
```

## ğŸ”§ **ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**

### ã‚ˆãã‚ã‚‹å•é¡Œ

#### 1. ãƒ¡ãƒ¢ãƒªä¸è¶³

```bash
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
python train.py --batch_size 4 --excel_file "data.xlsx"
```

#### 2. GPUä½¿ç”¨æ™‚ã®ã‚¨ãƒ©ãƒ¼

```bash
# CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
python -c "import torch; print(torch.version.cuda)"

# CPUå¼·åˆ¶ä½¿ç”¨
python train.py --excel_file "data.xlsx"  # --gpuã‚’ä»˜ã‘ãªã„
```

#### 3. æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼

```python
# matplotlibè¨­å®šã‚’ç¢ºèª
import matplotlib.pyplot as plt
print(plt.rcParams['font.family'])
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

#### GPUä½¿ç”¨æ™‚

```python
# config.pyã§è¨­å®š
DEVICE_CONFIG = {
    'use_gpu': True,
    'gpu_id': 0,
}

MODEL_CONFIG = {
    'batch_size': 16,  # GPUãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´
}
```

#### CPUä½¿ç”¨æ™‚

```python
MODEL_CONFIG = {
    'batch_size': 4,   # CPUã§ã¯å°ã•ã‚ã«
    'num_epochs': 3,   # ã‚¨ãƒãƒƒã‚¯æ•°ã‚’æ¸›ã‚‰ã™
}
```

## ğŸ“ **ãƒ­ã‚°ã¨ç›£è¦–**

### ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«

```bash
# æœ€æ–°ã®ãƒ­ã‚°ã‚’ç¢ºèª
tail -f logs/training_*.log

# ã‚¨ãƒ©ãƒ¼ã®ã¿è¡¨ç¤º
grep "ERROR" logs/training_*.log
```

### å­¦ç¿’ã®é€²æ—ç›£è¦–

```python
# å­¦ç¿’ä¸­ã®å‡ºåŠ›ä¾‹
2025-07-14 18:30:00 - INFO - å­¦ç¿’é–‹å§‹...
2025-07-14 18:30:15 - INFO - Epoch 1/5
2025-07-14 18:32:30 - INFO - Training Loss: 1.234
2025-07-14 18:32:30 - INFO - Validation F1: 0.856
```

## ğŸ”„ **ãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°ã¨å†å­¦ç¿’**

### æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§ã®è¿½åŠ å­¦ç¿’

```python
# æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
classifier = BERTClassifier()
classifier.load_saved_model("./models/")

# æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§è¿½åŠ å­¦ç¿’
# ï¼ˆå®Ÿè£…ã§ã¯æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¦å…¨ä½“ã‚’å†å­¦ç¿’ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ï¼‰
```

### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´

```python
# config.pyã‚’å¤‰æ›´ã™ã‚‹ã‹ã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æŒ‡å®š
python train.py --excel_file "data.xlsx" \\
                --learning_rate 5e-5 \\
                --batch_size 12 \\
                --epochs 8
```

## ğŸ§ª **å®Ÿé¨“ç®¡ç†**

### å®Ÿé¨“ã®å‘½åã¨ç®¡ç†

```bash
# å®Ÿé¨“åã‚’æŒ‡å®š
python train.py --experiment_name "baseline_v1" --excel_file "data.xlsx"
python train.py --experiment_name "high_lr_v1" --learning_rate 5e-5 --excel_file "data.xlsx"
python train.py --experiment_name "large_batch_v1" --batch_size 16 --excel_file "data.xlsx"
```

### çµæœã®æ¯”è¼ƒ

```python
import json

# è¤‡æ•°å®Ÿé¨“ã®çµæœã‚’æ¯”è¼ƒ
experiments = ['baseline_v1', 'high_lr_v1', 'large_batch_v1']

for exp in experiments:
    with open(f'results/{exp}.json', 'r') as f:
        result = json.load(f)
    
    accuracy = result['results']['training']['test_results']['eval_accuracy']
    f1 = result['results']['training']['test_results']['eval_f1']
    
    print(f"{exp}: ç²¾åº¦={accuracy:.3f}, F1={f1:.3f}")
```

## ğŸ“š **API ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹**

### DataPreprocessor

```python
preprocessor = DataPreprocessor()

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
processed_data = preprocessor.process_data(excel_file_path)

# å€‹åˆ¥ã®å‰å‡¦ç†æ©Ÿèƒ½
clean_text = preprocessor.clean_text("æ±šã„ãƒ†ã‚­ã‚¹ãƒˆ")
df = preprocessor.validate_data(raw_df)
```

### BERTClassifier

```python
classifier = BERTClassifier(model_name="custom-bert-model")

# å­¦ç¿’
results = classifier.train(processed_data, epochs=5)

# äºˆæ¸¬
prediction = classifier.predict(text, return_probabilities=True)
batch_predictions = classifier.predict_batch(text_list)

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿
classifier.save_model(path)
classifier.load_saved_model(path)
```

### EDAAnalyzer

```python
analyzer = EDAAnalyzer()

# å®Œå…¨ãªEDAå®Ÿè¡Œ
eda_results = analyzer.run_complete_eda(excel_file_path)

# å€‹åˆ¥ã®åˆ†æ
basic_stats = analyzer.analyze_basic_statistics(df)
analyzer.plot_label_distribution(df)
analyzer.create_wordcloud_by_label(df)
```

---

## ğŸ’¡ **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**

1. **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’**: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›ã§ç²¾åº¦å‘ä¸Š
2. **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã®BERTå­¦ç¿’
3. **APIåŒ–**: REST APIã¨ã—ã¦ãƒ‡ãƒ—ãƒ­ã‚¤
4. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬**: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®åˆ†é¡

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’åŸºç›¤ã¨ã—ã¦ã€ã•ã‚‰ãªã‚‹é«˜åº¦ãªæ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ãŒå¯èƒ½ã§ã™ï¼
