"""
å­¦ç¿’å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‹ã‚‰BERTå­¦ç¿’ã¾ã§ä¸€æ‹¬å®Ÿè¡Œ
"""
import os
import sys
import argparse
import logging
import json
from datetime import datetime
import torch

# ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

from data_preprocessing import DataPreprocessor
from eda import EDAAnalyzer
from bert_classifier import BERTClassifier
from config import MODEL_CONFIG, DATA_CONFIG, PATHS

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(os.path.join(PATHS['logs_dir'], f'training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'))
    ]
)
logger = logging.getLogger(__name__)

class TrainingPipeline:
    """å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_overrides: dict = None):
        self.config_overrides = config_overrides or {}
        self.results = {}
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        for path in PATHS.values():
            os.makedirs(path, exist_ok=True)
    
    def run_preprocessing(self, excel_file_path: str, run_eda: bool = True) -> dict:
        """
        ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨EDAã®å®Ÿè¡Œ
        
        Args:
            excel_file_path: Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            run_eda: EDAã‚’å®Ÿè¡Œã™ã‚‹ã‹ã©ã†ã‹
            
        Returns:
            dict: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
        """
        logger.info("=== ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†é–‹å§‹ ===")
        
        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        preprocessor = DataPreprocessor()
        processed_data = preprocessor.process_data(excel_file_path)
        
        # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
        logger.info(f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(processed_data['texts'])}")
        logger.info(f"ãƒ©ãƒ™ãƒ«æ•°: {len(processed_data['label_encoder'].classes_)}")
        logger.info(f"ãƒ©ãƒ™ãƒ«ä¸€è¦§: {list(processed_data['label_encoder'].classes_)}")
        
        # EDAã®å®Ÿè¡Œ
        if run_eda:
            logger.info("=== EDAåˆ†æé–‹å§‹ ===")
            eda_analyzer = EDAAnalyzer()
            eda_results = eda_analyzer.run_complete_eda(excel_file_path)
            processed_data['eda_results'] = eda_results
        
        # å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        preprocessor.save_processed_data(processed_data)
        
        self.results['preprocessing'] = {
            'data_stats': processed_data['data_stats'],
            'label_analysis': processed_data['label_analysis']
        }
        
        return processed_data
    
    def run_training(self, processed_data: dict, 
                    epochs: int = None, 
                    use_class_weights: bool = True,
                    model_name: str = None) -> dict:
        """
        BERTå­¦ç¿’ã®å®Ÿè¡Œ
        
        Args:
            processed_data: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
            epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            use_class_weights: ã‚¯ãƒ©ã‚¹é‡ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            model_name: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
            
        Returns:
            dict: å­¦ç¿’çµæœ
        """
        logger.info("=== BERTå­¦ç¿’é–‹å§‹ ===")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
        epochs = epochs or self.config_overrides.get('epochs', MODEL_CONFIG['num_epochs'])
        model_name = model_name or self.config_overrides.get('model_name', MODEL_CONFIG['bert_model_name'])
        
        # åˆ†é¡å™¨ã®åˆæœŸåŒ–
        classifier = BERTClassifier(model_name=model_name)
        
        # å­¦ç¿’å®Ÿè¡Œ
        training_results = classifier.train(
            processed_data=processed_data,
            epochs=epochs,
            use_class_weights=use_class_weights
        )
        
        # çµæœã®ä¿å­˜
        self.results['training'] = training_results
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’çµæœã«è¿½åŠ 
        training_results['model_info'] = {
            'model_name': model_name,
            'total_parameters': sum(p.numel() for p in classifier.model.parameters()),
            'trainable_parameters': sum(p.numel() for p in classifier.model.parameters() if p.requires_grad),
            'device': str(classifier.device)
        }
        
        logger.info(f"å­¦ç¿’å®Œäº† - ãƒ†ã‚¹ãƒˆç²¾åº¦: {training_results['test_results']['eval_accuracy']:.4f}")
        
        return training_results, classifier
    
    def evaluate_model(self, classifier: BERTClassifier, test_cases: list = None) -> dict:
        """
        ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°è©•ä¾¡
        
        Args:
            classifier: å­¦ç¿’æ¸ˆã¿åˆ†é¡å™¨
            test_cases: ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
            
        Returns:
            dict: è©•ä¾¡çµæœ
        """
        logger.info("=== ãƒ¢ãƒ‡ãƒ«è©•ä¾¡é–‹å§‹ ===")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        if test_cases is None:
            test_cases = [
                {
                    'text': 'ä»Šã¾ã§ãƒ‰ãƒ©ã‚¤ãƒ•ãƒ¼ãƒ‰ã‚’è‰²ã€…è©¦ã—ã¦ãã¾ã—ãŸãŒã€ã©ã‚Œã‚‚ã¡ã‚ƒã‚“ã¨é£Ÿã¹ãšã«æ®‹ã—ã¦ã¾ã—ãŸã€‚ã“ã¡ã‚‰ã¯åˆã‚ã¦ä¸ãˆãŸç¬é–“ã‹ã‚‰é£›ã³ä»˜ã„ã¦å–œã‚“ã§å®Œé£Ÿã—ã¦ãã‚Œã¾ã—ãŸã€‚',
                    'expected': 'é£Ÿã¹ã‚‹'
                },
                {
                    'text': 'ã†ã¡ã®çŒ«ã¯ã“ã‚Œã‚’é£Ÿã¹ã‚‹ã¨ã€ä¸‹ç—¢ã§ã—ãŸ',
                    'expected': 'åããƒ»ä¾¿ãŒæ‚ªããªã‚‹'
                },
                {
                    'text': 'é…é€ã®ç®±ãŒç ´ã‚Œã¦ã„ã¦ä¸­èº«ãŒã“ã¼ã‚Œã¦ã„ã¾ã—ãŸ',
                    'expected': 'é…é€ãƒ»æ¢±åŒ…'
                },
                {
                    'text': 'å€¤æ®µãŒé«˜ããªã£ã¦ãã¦å›°ã‚Šã¾ã™',
                    'expected': 'å€¤ä¸ŠãŒã‚Š/é«˜ã„'
                },
                {
                    'text': 'å®‰ãã¦åŠ©ã‹ã‚Šã¾ã™',
                    'expected': 'å®‰ã„'
                }
            ]
        
        evaluation_results = []
        
        for i, test_case in enumerate(test_cases):
            try:
                prediction = classifier.predict(
                    test_case['text'], 
                    return_probabilities=True
                )
                
                result = {
                    'test_id': i + 1,
                    'text': test_case['text'],
                    'expected': test_case.get('expected', 'Unknown'),
                    'predicted': prediction['predicted_label'],
                    'confidence': prediction['confidence'],
                    'correct': prediction['predicted_label'] == test_case.get('expected'),
                    'all_probabilities': prediction.get('all_probabilities', {})
                }
                
                evaluation_results.append(result)
                
                logger.info(f"ãƒ†ã‚¹ãƒˆ{i+1}: {result['predicted']} (ç¢ºä¿¡åº¦: {result['confidence']:.3f}) "
                          f"{'âœ“' if result['correct'] else 'âœ—'}")
                
            except Exception as e:
                logger.error(f"ãƒ†ã‚¹ãƒˆ{i+1}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                evaluation_results.append({
                    'test_id': i + 1,
                    'text': test_case['text'],
                    'error': str(e)
                })
        
        # æ­£è§£ç‡è¨ˆç®—
        correct_predictions = sum(1 for r in evaluation_results if r.get('correct', False))
        accuracy = correct_predictions / len(evaluation_results) if evaluation_results else 0
        
        eval_summary = {
            'test_cases': evaluation_results,
            'accuracy': accuracy,
            'total_tests': len(evaluation_results),
            'correct_predictions': correct_predictions
        }
        
        logger.info(f"ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ­£è§£ç‡: {accuracy:.3f} ({correct_predictions}/{len(evaluation_results)})")
        
        return eval_summary
    
    def save_experiment_results(self, experiment_name: str = None):
        """å®Ÿé¨“çµæœã®ä¿å­˜"""
        if experiment_name is None:
            experiment_name = f"experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # å®Ÿé¨“çµæœã‚’ã¾ã¨ã‚ã‚‹
        experiment_results = {
            'experiment_info': {
                'name': experiment_name,
                'timestamp': datetime.now().isoformat(),
                'config_overrides': self.config_overrides,
                'model_config': MODEL_CONFIG,
                'data_config': DATA_CONFIG
            },
            'results': self.results
        }
        
        # ä¿å­˜
        results_path = os.path.join(PATHS['results_dir'], f'{experiment_name}.json')
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(experiment_results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"å®Ÿé¨“çµæœã‚’ä¿å­˜: {results_path}")
        
        return experiment_results
    
    def run_complete_pipeline(self, excel_file_path: str, 
                            epochs: int = None,
                            run_eda: bool = True,
                            test_cases: list = None,
                            experiment_name: str = None) -> dict:
        """
        å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        
        Args:
            excel_file_path: Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            run_eda: EDAã‚’å®Ÿè¡Œã™ã‚‹ã‹
            test_cases: ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
            experiment_name: å®Ÿé¨“å
            
        Returns:
            dict: å®Œå…¨ãªå®Ÿé¨“çµæœ
        """
        logger.info("=== å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹ ===")
        
        start_time = datetime.now()
        
        try:
            # 1. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨EDA
            processed_data = self.run_preprocessing(excel_file_path, run_eda)
            
            # 2. BERTå­¦ç¿’
            training_results, classifier = self.run_training(
                processed_data, epochs=epochs
            )
            
            # 3. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
            evaluation_results = self.evaluate_model(classifier, test_cases)
            self.results['evaluation'] = evaluation_results
            
            # 4. å®Ÿé¨“çµæœã®ä¿å­˜
            experiment_results = self.save_experiment_results(experiment_name)
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            logger.info(f"=== ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº† ===")
            logger.info(f"å®Ÿè¡Œæ™‚é–“: {duration}")
            logger.info(f"æœ€çµ‚ãƒ†ã‚¹ãƒˆç²¾åº¦: {training_results['test_results']['eval_accuracy']:.4f}")
            logger.info(f"ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ­£è§£ç‡: {evaluation_results['accuracy']:.3f}")
            
            return experiment_results
            
        except Exception as e:
            logger.error(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            raise

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='ãƒšãƒƒãƒˆãƒ•ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ†é¡å™¨ã®å­¦ç¿’')
    
    parser.add_argument('--excel_file', type=str, required=True,
                       help='å­¦ç¿’ç”¨Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--epochs', type=int, default=5,
                       help='ã‚¨ãƒãƒƒã‚¯æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5)')
    parser.add_argument('--batch_size', type=int, default=8,
                       help='ãƒãƒƒãƒã‚µã‚¤ã‚º (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 8)')
    parser.add_argument('--learning_rate', type=float, default=2e-5,
                       help='å­¦ç¿’ç‡ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2e-5)')
    parser.add_argument('--model_name', type=str, 
                       default='tohoku-nlp/bert-base-japanese-whole-word-masking',
                       help='ä½¿ç”¨ã™ã‚‹BERTãƒ¢ãƒ‡ãƒ«å')
    parser.add_argument('--experiment_name', type=str,
                       help='å®Ÿé¨“å')
    parser.add_argument('--skip_eda', action='store_true',
                       help='EDAã‚’ã‚¹ã‚­ãƒƒãƒ—')
    parser.add_argument('--gpu', action='store_true',
                       help='GPUã‚’ä½¿ç”¨')
    
    args = parser.parse_args()
    
    # è¨­å®šã®ä¸Šæ›¸ã
    config_overrides = {
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'learning_rate': args.learning_rate,
        'model_name': args.model_name,
        'use_gpu': args.gpu
    }
    
    # MODEL_CONFIGã‚’å‹•çš„ã«æ›´æ–°
    if args.batch_size != MODEL_CONFIG['batch_size']:
        MODEL_CONFIG['batch_size'] = args.batch_size
    if args.learning_rate != MODEL_CONFIG['learning_rate']:
        MODEL_CONFIG['learning_rate'] = args.learning_rate
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
    if not os.path.exists(args.excel_file):
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.excel_file}")
        return 1
    
    # GPUè¨­å®š
    if args.gpu and torch.cuda.is_available():
        logger.info(f"GPUä½¿ç”¨: {torch.cuda.get_device_name()}")
    else:
        logger.info("CPUä½¿ç”¨")
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    pipeline = TrainingPipeline(config_overrides)
    
    try:
        results = pipeline.run_complete_pipeline(
            excel_file_path=args.excel_file,
            epochs=args.epochs,
            run_eda=not args.skip_eda,
            experiment_name=args.experiment_name
        )
        
        print("\\n" + "="*50)
        print("ğŸ‰ å­¦ç¿’å®Œäº†ï¼")
        print("="*50)
        print(f"å®Ÿé¨“å: {results['experiment_info']['name']}")
        print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {results['results']['training']['test_results']['eval_accuracy']:.4f}")
        print(f"ãƒ†ã‚¹ãƒˆF1: {results['results']['training']['test_results']['eval_f1']:.4f}")
        print(f"ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ­£è§£ç‡: {results['results']['evaluation']['accuracy']:.3f}")
        print(f"çµæœä¿å­˜å…ˆ: {PATHS['results_dir']}")
        print(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {PATHS['models_dir']}")
        
        return 0
        
    except Exception as e:
        logger.error(f"å®Ÿè¡Œå¤±æ•—: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
